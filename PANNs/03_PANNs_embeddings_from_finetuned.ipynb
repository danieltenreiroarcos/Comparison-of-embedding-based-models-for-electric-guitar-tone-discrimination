{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880284d9",
   "metadata": {},
   "source": [
    "# PANNs Revisados: Embeddings From Fine-Tuned Model\n",
    "\n",
    "Carga `best.pt` y genera embeddings en disco, sin re-entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74420c",
   "metadata": {},
   "source": [
    "# 1. Configuración\n",
    "\n",
    "Este notebook replica el flujo del baseline de PANNs, pero insertando un bloque de **fine-tuning** más parecido al de MERT: además de una cabeza de proyección para embeddings, se permite **adaptación parcial del backbone** (últimos bloques convolucionales) para especializar el modelo al dominio EGFxSet.\n",
    "\n",
    "Objetivos:\n",
    "\n",
    "- Medir tiempo total de entrenamiento.\n",
    "- Mantener protocolo de evaluación idéntico (Top‑1/Top‑5 por similitud coseno).\n",
    "- Comparar con: (i) PANNs preentrenado, (ii) fine-tuning con cabeza congelando backbone, (iii) este experimento con backbone parcialmente entrenable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb46fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mert310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c957ac6a-f672-4848-9873-5481d7c442b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists: True -> ../EGFxSet\n",
      "OUT_DIR exists: True -> outputs_01_panns_base_egfxset\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"../EGFxSet\")\n",
    "OUT_DIR   = Path(\"./outputs_01_panns_base_egfxset\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT exists:\", DATA_ROOT.exists(), \"->\", DATA_ROOT)\n",
    "print(\"OUT_DIR exists:\", OUT_DIR.exists(), \"->\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a38a7d",
   "metadata": {},
   "source": [
    "## 2. Semilla y carpeta de salida\n",
    "\n",
    "Fija semilla (reproducibilidad) y define el directorio de salida de este experimento (02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6fbff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR_03: /Users/dtenreiro/Documents/TFM/panns_inference/outputs_03_panns_finetune_egfxset_unfreeze\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "OUT_DIR_03 = Path(\"./outputs_03_panns_finetune_egfxset_unfreeze\")\n",
    "OUT_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(\"OUT_DIR_03:\", OUT_DIR_03.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7f7f5",
   "metadata": {},
   "source": [
    "## 3. Dispositivo\n",
    "\n",
    "Selecciona `mps/cuda/cpu` igual que en el baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b65aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon MPS enabled\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "elif device == \"mps\":\n",
    "    print(\"Apple Silicon MPS enabled\")\n",
    "else:\n",
    "    print(\"CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374cfcf",
   "metadata": {},
   "source": [
    "# 2. Carga del índice del dataset\n",
    "\n",
    "Reutiliza el `egfxset_index.csv` generado por el baseline de PANNs para mantener el **mismo orden** y metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3124132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: (8947, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>tone</th>\n",
       "      <th>pickup</th>\n",
       "      <th>tone_pickup</th>\n",
       "      <th>string</th>\n",
       "      <th>fret</th>\n",
       "      <th>midi_pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path         tone  pickup  \\\n",
       "0  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "1  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "2  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "3  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "4  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "\n",
       "           tone_pickup  string  fret  midi_pitch  \n",
       "0  BluesDriver__Bridge       1     0          64  \n",
       "1  BluesDriver__Bridge       1     1          65  \n",
       "2  BluesDriver__Bridge       1    10          74  \n",
       "3  BluesDriver__Bridge       1    11          75  \n",
       "4  BluesDriver__Bridge       1    12          76  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INDEX_CSV_PATH = Path(\"../egfxset_index.csv\")\n",
    "assert INDEX_CSV_PATH.exists(), f\"No encuentro {INDEX_CSV_PATH}. Ejecuta antes el notebook 01_PANNs_base_egfx.\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV_PATH)\n",
    "print(\"Index:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33235c1",
   "metadata": {},
   "source": [
    "## 1. Configuración del objetivo de fine-tuning\n",
    "\n",
    "La tarea de fine-tuning será **clasificar `tone_pickup`** (65 clases), como en el fine-tuning de MERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9e46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CLASSES: 65\n",
      "Train: (7157, 9) | Val: (1790, 9)\n",
      "train_idx: (7157,) | val_idx: (1790,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "LABEL_COL = \"tone_pickup\"\n",
    "\n",
    "df[\"row_id\"] = np.arange(len(df), dtype=int)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[LABEL_COL].astype(str))\n",
    "N_CLASSES = len(le.classes_)\n",
    "print(\"N_CLASSES:\", N_CLASSES)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "\n",
    "train_idx = train_df[\"row_id\"].to_numpy()\n",
    "val_idx   = val_df[\"row_id\"].to_numpy()\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"| Val:\", val_df.shape)\n",
    "print(\"train_idx:\", train_idx.shape, \"| val_idx:\", val_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb722",
   "metadata": {},
   "source": [
    "# 3. PANNs: carga y fine-tuning\n",
    "\n",
    "Cargamos PANNs CNN14 preentrenado (igual que en el baseline) y definimos una cabeza de clasificación para `tone_pickup`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786615c",
   "metadata": {},
   "source": [
    "## 1. Parámetros de audio y utilidades de pooling\n",
    "\n",
    "Se mantiene el mismo preprocesado (32 kHz, recorte/padding a 5 s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8d4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECONDS = 5.0\n",
    "TARGET_SR = 32000\n",
    "\n",
    "def load_audio_panns(path: str | Path):\n",
    "    \"\"\"Carga wav -> mono -> resample a 32kHz -> recorta a 5s\"\"\"\n",
    "    wav, sr = sf.read(str(path))\n",
    "\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "\n",
    "    wav = torch.from_numpy(wav).float()\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "        sr = TARGET_SR\n",
    "\n",
    "    wav = wav[: int(sr * MAX_SECONDS)]\n",
    "    return wav, sr\n",
    "\n",
    "def temporal_pool(x: torch.Tensor, mode: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (T, D) ó (B, T, D). Devuelve (D) ó (B, D)\n",
    "    \"\"\"\n",
    "    if x.dim() == 2:\n",
    "        x0 = x\n",
    "        if mode == \"mean\":\n",
    "            return x0.mean(dim=0)\n",
    "        elif mode == \"max\":\n",
    "            return x0.max(dim=0).values\n",
    "        elif mode == \"stats\":\n",
    "            mu = x0.mean(dim=0)\n",
    "            sd = x0.std(dim=0, unbiased=False)\n",
    "            return torch.cat([mu, sd], dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    if x.dim() == 3:\n",
    "        if mode == \"mean\":\n",
    "            return x.mean(dim=1)\n",
    "        elif mode == \"max\":\n",
    "            return x.max(dim=1).values\n",
    "        elif mode == \"stats\":\n",
    "            mu = x.mean(dim=1)\n",
    "            sd = x.std(dim=1, unbiased=False)\n",
    "            return torch.cat([mu, sd], dim=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    raise ValueError(f\"Unexpected tensor rank: {x.dim()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274ba66",
   "metadata": {},
   "source": [
    "## 2. Dataset + DataLoaders\n",
    "\n",
    "Construye un `Dataset` y un `collate_fn` que:\n",
    "- Carga WAV\n",
    "- Convierte a mono\n",
    "- Re-muestrea a 32 kHz\n",
    "- Recorta o **rellena con ceros** hasta 5 s\n",
    "\n",
    "Esto permite hacer batches con tensores de longitud fija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b5fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_COL = \"path\"\n",
    "LABEL_ID_COL = \"label_id\"\n",
    "\n",
    "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
    "\n",
    "def load_audio_fixed(path: str | Path):\n",
    "    wav, sr = load_audio_panns(path)\n",
    "\n",
    "    if wav.numel() < MAX_LEN:\n",
    "        pad = MAX_LEN - wav.numel()\n",
    "        wav = torch.nn.functional.pad(wav, (0, pad))\n",
    "    else:\n",
    "        wav = wav[:MAX_LEN]\n",
    "    return wav\n",
    "\n",
    "class EGFxSetToneDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav = load_audio_fixed(row[PATH_COL])\n",
    "        y = int(row[LABEL_ID_COL])\n",
    "        return wav, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    wavs, ys = zip(*batch)\n",
    "    x = torch.stack([w.float() for w in wavs], dim=0)  # (B, T)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(EGFxSetToneDataset(train_df), batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(EGFxSetToneDataset(val_df),   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34385cf2",
   "metadata": {},
   "source": [
    "## 3. Cargar PANNs CNN14 + checkpoint\n",
    "\n",
    "Reutiliza el mismo bloque del baseline para localizar y cargar el `.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d87c08e-f31f-4d9b-bf44-2688e2be1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: pretrained_models/Cnn14_mAP=0.431.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "OUT = Path(\"pretrained_models\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = \"https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\"\n",
    "ckpt = OUT / \"Cnn14_mAP=0.431.pth\"\n",
    "\n",
    "if not ckpt.exists():\n",
    "    urllib.request.urlretrieve(url, ckpt)\n",
    "\n",
    "PANNS_CNN14_CKPT = str(ckpt)\n",
    "print(\"Checkpoint:\", PANNS_CNN14_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5642457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /Users/dtenreiro/Documents/TFM/panns_inference/pretrained_models/Cnn14_mAP=0.431.pth\n",
      "Loaded PANNs: Cnn14 on mps\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import inspect\n",
    "import torch\n",
    "from panns_inference.models import Cnn14\n",
    "\n",
    "def find_cnn14_ckpt() -> Path:\n",
    "\n",
    "    env = os.environ.get(\"PANNS_CNN14_CKPT\", \"\").strip()\n",
    "    if env:\n",
    "        p = Path(env).expanduser()\n",
    "        if p.exists() and p.is_file():\n",
    "            return p\n",
    "        raise FileNotFoundError(f\"PANNS_CNN14_CKPT apunta a algo que no es fichero: {p}\")\n",
    "\n",
    "\n",
    "    pkg_root = Path(inspect.getfile(Cnn14)).resolve().parent\n",
    "    candidates = []\n",
    "    for rel in [\n",
    "        \"../pretrained_models\",\n",
    "        \"../pretrained\",\n",
    "        \"../data\",\n",
    "        \"../../pretrained_models\",\n",
    "        \"../../pretrained\",\n",
    "    ]:\n",
    "        d = (pkg_root / rel).resolve()\n",
    "        if d.exists() and d.is_dir():\n",
    "            candidates += list(d.rglob(\"*.pth\"))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda p: ((\"cnn14\" not in p.name.lower()), len(p.name)))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"No se ha encontrado ningún checkpoint .pth de PANNs CNN14.\\n\"\n",
    "    )\n",
    "\n",
    "CKPT = find_cnn14_ckpt()\n",
    "print(\"Using checkpoint:\", CKPT)\n",
    "\n",
    "panns_model = Cnn14(\n",
    "    sample_rate=TARGET_SR,\n",
    "    window_size=1024,\n",
    "    hop_size=320,\n",
    "    mel_bins=64,\n",
    "    fmin=50,\n",
    "    fmax=14000,\n",
    "    classes_num=527\n",
    ")\n",
    "\n",
    "ckpt = torch.load(CKPT, map_location=device)\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "panns_model.load_state_dict(state, strict=True)\n",
    "\n",
    "panns_model.to(device).eval()\n",
    "print(\"Loaded PANNs:\", panns_model.__class__.__name__, \"on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411656a",
   "metadata": {},
   "source": [
    "## 4. Modelo para fine-tuning\n",
    "\n",
    "Crea un wrapper que:\n",
    "- Obtiene el embedding global de PANNs (`out['embedding']`).\n",
    "- Aplica una capa `Linear` para predecir `tone_pickup`.\n",
    "\n",
    "Así podemos entrenar una cabeza ligera y, opcionalmente, descongelar una parte final del encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e9695",
   "metadata": {},
   "source": [
    "## Fine-tuning con *projection head* + adaptación parcial del backbone\n",
    "\n",
    "**Objetivo**: mejorar la calidad del espacio de embeddings para *retrieval* (Top‑1/Top‑5) manteniendo el pooling interno de PANNs, pero permitiendo una **adaptación controlada** de las **últimas capas** del encoder.\n",
    "\n",
    "**Qué cambia respecto al notebook anterior**:\n",
    "- Además de la proyección `2048 → 256`, se **descongelan** los últimos bloques convolucionales para que el modelo pueda especializarse al dominio EGFxSet (estrategia más cercana al fine‑tuning parcial aplicado en MERT).\n",
    "- El resto del protocolo (split, ventanas, evaluación por similitud coseno) se mantiene idéntico para comparar de forma directa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9069c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_dim: 2048 | proj_dim: 256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PANNsForTonePickup(nn.Module):\n",
    "    \"\"\"Wrapper de fine-tuning para PANNs.\n",
    "\n",
    "    - backbone: PANNs (CNN14) preentrenado\n",
    "    - proj: proyección entrenable para compactar el embedding (mejor retrieval)\n",
    "    - classifier: capa final de clasificación\n",
    "\n",
    "    Nota: el pooling temporal interno de PANNs NO se modifica; aquí solo se adapta el embedding final.\n",
    "    \"\"\"\n",
    "    def __init__(self, panns_backbone: nn.Module, emb_dim: int, proj_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.panns = panns_backbone\n",
    "        self.proj = nn.Linear(emb_dim, proj_dim)\n",
    "        self.classifier = nn.Linear(proj_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.panns(x, None)\n",
    "        emb_raw = out[\"embedding\"]\n",
    "        emb = self.proj(emb_raw)\n",
    "        emb = F.normalize(emb, p=2, dim=-1)\n",
    "        logits = self.classifier(emb)\n",
    "        return logits, emb, emb_raw\n",
    "\n",
    "panns_model.eval()\n",
    "with torch.no_grad():\n",
    "    x0, _ = next(iter(train_loader))\n",
    "    x0 = x0.to(device)\n",
    "    out0 = panns_model(x0, None)\n",
    "    emb_dim = int(out0[\"embedding\"].shape[-1])\n",
    "\n",
    "PROJ_DIM = 256\n",
    "ft_model = PANNsForTonePickup(panns_model, emb_dim=emb_dim, proj_dim=PROJ_DIM, n_classes=N_CLASSES).to(device)\n",
    "print(\"emb_dim:\", emb_dim, \"| proj_dim:\", PROJ_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a5740",
   "metadata": {},
   "source": [
    "## Cargar checkpoint fine-tuned guardado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63bfb68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned checkpoint: outputs_03_panns_finetune_egfxset_unfreeze/best.pt\n",
      "best_epoch: 30 | best_val_loss: 0.31401769463909407\n"
     ]
    }
   ],
   "source": [
    "BEST_PT = OUT_DIR_03 / \"best.pt\"\n",
    "assert BEST_PT.exists(), f\"No existe checkpoint fine-tuned: {BEST_PT}\"\n",
    "\n",
    "ckpt_ft = torch.load(BEST_PT, map_location=device)\n",
    "state_dict = ckpt_ft.get(\"state_dict\", ckpt_ft)\n",
    "ft_model.load_state_dict(state_dict, strict=True)\n",
    "ft_model.eval()\n",
    "\n",
    "print(\"Loaded fine-tuned checkpoint:\", BEST_PT)\n",
    "print(\"best_epoch:\", ckpt_ft.get(\"best_epoch\", None), \"| best_val_loss:\", ckpt_ft.get(\"best_val_loss\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913838a9",
   "metadata": {},
   "source": [
    "## Guardar snapshots de split (trazabilidad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd31c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_03_panns_finetune_egfxset_unfreeze/egfxset_index_used_train.csv\n",
      "Saved: outputs_03_panns_finetune_egfxset_unfreeze/egfxset_index_used_val.csv\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(OUT_DIR_03 / \"egfxset_index_used_train.csv\", index=False)\n",
    "val_df.to_csv(OUT_DIR_03 / \"egfxset_index_used_val.csv\", index=False)\n",
    "print(\"Saved:\", OUT_DIR_03 / \"egfxset_index_used_train.csv\")\n",
    "print(\"Saved:\", OUT_DIR_03 / \"egfxset_index_used_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e80c2",
   "metadata": {},
   "source": [
    "## 2. Función de extracción de embeddings (fine-tuned)\n",
    "\n",
    "Mismo formato que el baseline, pero usando el backbone fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15be2af",
   "metadata": {},
   "source": [
    "## Extracción de embeddings (para evaluación Top‑1/Top‑5)\n",
    "\n",
    "Guardamos dos variantes:\n",
    "- `global_raw`: embedding global original de PANNs (2048)\n",
    "- `global_proj`: embedding proyectado y normalizado (256) -> **el candidato principal** para retrieval\n",
    "\n",
    "Opcionalmente mantenemos las variantes framewise (si existen) usando el framewise output del backbone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f70c9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_panns_variants_ft(path):\n",
    "    \"\"\"Devuelve dict {variant: embedding_1D_cpu}.\"\"\"\n",
    "    x, sr = load_audio_panns(path)\n",
    "    x = x.to(device).unsqueeze(0)  # (1, n)\n",
    "\n",
    "    variants = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, emb_proj, emb_raw = ft_model(x)\n",
    "\n",
    "        out_backbone = ft_model.panns(x, None)\n",
    "\n",
    "    variants[\"global_raw\"]  = emb_raw.squeeze(0).float().cpu()\n",
    "    variants[\"global_proj\"] = emb_proj.squeeze(0).float().cpu()\n",
    "\n",
    "    frame = None\n",
    "    if \"framewise_embedding\" in out_backbone:\n",
    "        frame = out_backbone[\"framewise_embedding\"]  # (B, T, D)\n",
    "    elif \"framewise_output\" in out_backbone:\n",
    "        frame = out_backbone[\"framewise_output\"]     # (B, T, D) o (B, T, C)\n",
    "\n",
    "    if frame is not None:\n",
    "        frame = frame.squeeze(0).float().cpu()  # (T, D)\n",
    "        variants[\"frame_mean\"]  = temporal_pool(frame, \"mean\")\n",
    "        variants[\"frame_max\"]   = temporal_pool(frame, \"max\")\n",
    "        variants[\"frame_stats\"] = temporal_pool(frame, \"stats\")\n",
    "\n",
    "    return variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eeb013",
   "metadata": {},
   "source": [
    "# 5. Embeddings tras fine-tuning\n",
    "\n",
    "Extrae embeddings para **todo** el dataset, guardándolos en memmaps dentro de `outputs_02_...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414e810",
   "metadata": {},
   "source": [
    "## Generación masiva de embeddings en disco\n",
    "\n",
    "Se generan ficheros `.npy` para todas las variantes retornadas por `extract_panns_variants_ft`.\n",
    "En este experimento, la variante clave es `global_proj` (256), ya normalizada L2 en el `forward`.\n",
    "Aun así, en la evaluación aplicamos una normalización L2 adicional por seguridad (no hace daño).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d0d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants/dims: {'global_raw': 2048, 'global_proj': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PANNs FT embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8946/8946 [01:44<00:00, 85.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_03_panns_finetune_egfxset_unfreeze/embeddings_panns_ft_proj_global_raw.npy\n",
      "Saved: outputs_03_panns_finetune_egfxset_unfreeze/embeddings_panns_ft_proj_global_proj.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "paths = df[\"path\"].tolist()\n",
    "N = len(paths)\n",
    "\n",
    "test_vars = extract_panns_variants_ft(paths[0])\n",
    "dims = {k: v.numel() for k, v in test_vars.items()}\n",
    "print(\"Variants/dims:\", dims)\n",
    "\n",
    "out_memmaps = {}\n",
    "for variant, dim in dims.items():\n",
    "    out_path = OUT_DIR_03 / f\"embeddings_panns_ft_proj_{variant}.npy\"\n",
    "    mm = np.memmap(out_path, dtype=np.float32, mode=\"w+\", shape=(N, dim))\n",
    "    out_memmaps[variant] = (mm, out_path)\n",
    "\n",
    "for variant, vec in test_vars.items():\n",
    "    out_memmaps[variant][0][0] = vec.numpy().astype(np.float32)\n",
    "\n",
    "for i, path in enumerate(tqdm(paths[1:], desc=\"Extracting PANNs FT embeddings\"), start=1):\n",
    "    vars_i = extract_panns_variants_ft(path)\n",
    "    for variant in dims.keys():\n",
    "        vec = vars_i.get(variant, vars_i.get(\"global_proj\", vars_i[\"global_raw\"]))\n",
    "        out_memmaps[variant][0][i] = vec.numpy().astype(np.float32)\n",
    "\n",
    "for variant, (mm, out_path) in out_memmaps.items():\n",
    "    mm.flush()\n",
    "    print(\"Saved:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mert310)",
   "language": "python",
   "name": "mert310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
