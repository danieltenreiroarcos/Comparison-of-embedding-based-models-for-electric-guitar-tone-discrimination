{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c74420c",
   "metadata": {},
   "source": [
    "# 1. Configuración\n",
    "\n",
    "Este notebook replica el flujo del baseline de PANNs, pero insertando un bloque de **fine-tuning** más parecido al de MERT: además de una cabeza de proyección para embeddings, se permite **adaptación parcial del backbone** (últimos bloques convolucionales) para especializar el modelo al dominio EGFxSet.\n",
    "\n",
    "Objetivos:\n",
    "\n",
    "- Medir tiempo total de entrenamiento.\n",
    "- Mantener protocolo de evaluación idéntico (Top‑1/Top‑5 por similitud coseno).\n",
    "- Comparar con: (i) PANNs preentrenado, (ii) fine-tuning con cabeza congelando backbone, (iii) este experimento con backbone parcialmente entrenable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ab629",
   "metadata": {},
   "source": [
    "## 1. Imports centralizados\n",
    "\n",
    "Importa librerías base y define utilidades comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb46fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mert310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c957ac6a-f672-4848-9873-5481d7c442b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists: True -> /Users/dtenreiro/Documents/TFM/EGFxSet\n",
      "OUT_DIR exists: True -> outputs_01_panns_base_egfxset\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"/Users/dtenreiro/Documents/TFM/EGFxSet\")\n",
    "OUT_DIR   = Path(\"./outputs_01_panns_base_egfxset\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT exists:\", DATA_ROOT.exists(), \"->\", DATA_ROOT)\n",
    "print(\"OUT_DIR exists:\", OUT_DIR.exists(), \"->\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a38a7d",
   "metadata": {},
   "source": [
    "## 2. Semilla y carpeta de salida\n",
    "\n",
    "Fija semilla (reproducibilidad) y define el directorio de salida de este experimento (02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6fbff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR_03: /Users/dtenreiro/Documents/TFM/panns_inference/outputs_03_panns_finetune_egfxset_unfreeze\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "OUT_DIR_03 = Path(\"./outputs_03_panns_finetune_egfxset_unfreeze\")\n",
    "OUT_DIR_03.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(\"OUT_DIR_03:\", OUT_DIR_03.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7f7f5",
   "metadata": {},
   "source": [
    "## 3. Dispositivo\n",
    "\n",
    "Selecciona `mps/cuda/cpu` igual que en el baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b65aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon MPS enabled\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "elif device == \"mps\":\n",
    "    print(\"Apple Silicon MPS enabled\")\n",
    "else:\n",
    "    print(\"CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78223e51",
   "metadata": {},
   "source": [
    "## 4. Sincronización para benchmarks\n",
    "\n",
    "Define `_sync()` para que las medidas de tiempo sean correctas en GPU/MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c6adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _sync():\n",
    "    # Para que el cronómetro mida de verdad en GPU/MPS\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    elif device == \"mps\" and hasattr(torch, \"mps\"):\n",
    "        try:\n",
    "            torch.mps.synchronize()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _summ(times_s, label=\"\"):\n",
    "    times = np.array(times_s, dtype=np.float64)\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"n\": int(times.size),\n",
    "        \"mean_ms\": float(times.mean() * 1000),\n",
    "        \"p50_ms\": float(np.percentile(times, 50) * 1000),\n",
    "        \"p95_ms\": float(np.percentile(times, 95) * 1000),\n",
    "        \"min_ms\": float(times.min() * 1000),\n",
    "        \"max_ms\": float(times.max() * 1000),\n",
    "        \"runs_per_s\": float(1.0 / times.mean()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374cfcf",
   "metadata": {},
   "source": [
    "# 2. Carga del índice del dataset\n",
    "\n",
    "Reutiliza el `egfxset_index.csv` generado por el baseline de PANNs para mantener el **mismo orden** y metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3124132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: (8947, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>tone</th>\n",
       "      <th>pickup</th>\n",
       "      <th>tone_pickup</th>\n",
       "      <th>string</th>\n",
       "      <th>fret</th>\n",
       "      <th>midi_pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path         tone  pickup  \\\n",
       "0  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "1  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "2  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "3  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "4  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "\n",
       "           tone_pickup  string  fret  midi_pitch  \n",
       "0  BluesDriver__Bridge       1     0          64  \n",
       "1  BluesDriver__Bridge       1     1          65  \n",
       "2  BluesDriver__Bridge       1    10          74  \n",
       "3  BluesDriver__Bridge       1    11          75  \n",
       "4  BluesDriver__Bridge       1    12          76  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INDEX_CSV_PATH = Path(\"./outputs_01_panns_base_egfxset/egfxset_index.csv\")\n",
    "assert INDEX_CSV_PATH.exists(), f\"No encuentro {INDEX_CSV_PATH}. Ejecuta antes el notebook 01_PANNs_base_egfx.\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV_PATH)\n",
    "print(\"Index:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33235c1",
   "metadata": {},
   "source": [
    "## 1. Configuración del objetivo de fine-tuning\n",
    "\n",
    "La tarea de fine-tuning será **clasificar `tone_pickup`** (65 clases), como en el fine-tuning de MERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9e46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CLASSES: 65\n",
      "Train: (7157, 9) | Val: (1790, 9)\n",
      "train_idx: (7157,) | val_idx: (1790,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "LABEL_COL = \"tone_pickup\"\n",
    "\n",
    "df[\"row_id\"] = np.arange(len(df), dtype=int)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[LABEL_COL].astype(str))\n",
    "N_CLASSES = len(le.classes_)\n",
    "print(\"N_CLASSES:\", N_CLASSES)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "\n",
    "train_idx = train_df[\"row_id\"].to_numpy()\n",
    "val_idx   = val_df[\"row_id\"].to_numpy()\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"| Val:\", val_df.shape)\n",
    "print(\"train_idx:\", train_idx.shape, \"| val_idx:\", val_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb722",
   "metadata": {},
   "source": [
    "# 3. PANNs: carga y fine-tuning\n",
    "\n",
    "Cargamos PANNs CNN14 preentrenado (igual que en el baseline) y definimos una cabeza de clasificación para `tone_pickup`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786615c",
   "metadata": {},
   "source": [
    "## 1. Parámetros de audio y utilidades de pooling\n",
    "\n",
    "Se mantiene el mismo preprocesado (32 kHz, recorte/padding a 5 s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8d4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECONDS = 5.0\n",
    "TARGET_SR = 32000\n",
    "\n",
    "def load_audio_panns(path: str | Path):\n",
    "    \"\"\"Carga wav -> mono -> resample a 32kHz -> recorta a 5s\"\"\"\n",
    "    wav, sr = sf.read(str(path))\n",
    "\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "\n",
    "    wav = torch.from_numpy(wav).float()\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "        sr = TARGET_SR\n",
    "\n",
    "    wav = wav[: int(sr * MAX_SECONDS)]\n",
    "    return wav, sr\n",
    "\n",
    "def temporal_pool(x: torch.Tensor, mode: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (T, D) ó (B, T, D). Devuelve (D) ó (B, D)\n",
    "    \"\"\"\n",
    "    if x.dim() == 2:\n",
    "        x0 = x\n",
    "        if mode == \"mean\":\n",
    "            return x0.mean(dim=0)\n",
    "        elif mode == \"max\":\n",
    "            return x0.max(dim=0).values\n",
    "        elif mode == \"stats\":\n",
    "            mu = x0.mean(dim=0)\n",
    "            sd = x0.std(dim=0, unbiased=False)\n",
    "            return torch.cat([mu, sd], dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    if x.dim() == 3:\n",
    "        if mode == \"mean\":\n",
    "            return x.mean(dim=1)\n",
    "        elif mode == \"max\":\n",
    "            return x.max(dim=1).values\n",
    "        elif mode == \"stats\":\n",
    "            mu = x.mean(dim=1)\n",
    "            sd = x.std(dim=1, unbiased=False)\n",
    "            return torch.cat([mu, sd], dim=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    raise ValueError(f\"Unexpected tensor rank: {x.dim()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274ba66",
   "metadata": {},
   "source": [
    "## 2. Dataset + DataLoaders\n",
    "\n",
    "Construye un `Dataset` y un `collate_fn` que:\n",
    "- Carga WAV\n",
    "- Convierte a mono\n",
    "- Re-muestrea a 32 kHz\n",
    "- Recorta o **rellena con ceros** hasta 5 s\n",
    "\n",
    "Esto permite hacer batches con tensores de longitud fija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b5fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_COL = \"path\"\n",
    "LABEL_ID_COL = \"label_id\"\n",
    "\n",
    "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
    "\n",
    "def load_audio_fixed(path: str | Path):\n",
    "    wav, sr = load_audio_panns(path)\n",
    "\n",
    "    if wav.numel() < MAX_LEN:\n",
    "        pad = MAX_LEN - wav.numel()\n",
    "        wav = torch.nn.functional.pad(wav, (0, pad))\n",
    "    else:\n",
    "        wav = wav[:MAX_LEN]\n",
    "    return wav\n",
    "\n",
    "class EGFxSetToneDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav = load_audio_fixed(row[PATH_COL])\n",
    "        y = int(row[LABEL_ID_COL])\n",
    "        return wav, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    wavs, ys = zip(*batch)\n",
    "    x = torch.stack([w.float() for w in wavs], dim=0)  # (B, T)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(EGFxSetToneDataset(train_df), batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(EGFxSetToneDataset(val_df),   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34385cf2",
   "metadata": {},
   "source": [
    "## 3. Cargar PANNs CNN14 + checkpoint\n",
    "\n",
    "Reutiliza el mismo bloque del baseline para localizar y cargar el `.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d87c08e-f31f-4d9b-bf44-2688e2be1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: pretrained_models/Cnn14_mAP=0.431.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "OUT = Path(\"pretrained_models\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = \"https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\"\n",
    "ckpt = OUT / \"Cnn14_mAP=0.431.pth\"\n",
    "\n",
    "if not ckpt.exists():\n",
    "    urllib.request.urlretrieve(url, ckpt)\n",
    "\n",
    "PANNS_CNN14_CKPT = str(ckpt)\n",
    "print(\"Checkpoint:\", PANNS_CNN14_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5642457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /Users/dtenreiro/Documents/TFM/panns_inference/pretrained_models/Cnn14_mAP=0.431.pth\n",
      "Loaded PANNs: Cnn14 on mps\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import inspect\n",
    "import torch\n",
    "from panns_inference.models import Cnn14\n",
    "\n",
    "def find_cnn14_ckpt() -> Path:\n",
    "\n",
    "    env = os.environ.get(\"PANNS_CNN14_CKPT\", \"\").strip()\n",
    "    if env:\n",
    "        p = Path(env).expanduser()\n",
    "        if p.exists() and p.is_file():\n",
    "            return p\n",
    "        raise FileNotFoundError(f\"PANNS_CNN14_CKPT not found: {p}\")\n",
    "\n",
    "    pkg_root = Path(inspect.getfile(Cnn14)).resolve().parent\n",
    "    candidates = []\n",
    "    for rel in [\n",
    "        \"../pretrained_models\",\n",
    "        \"../pretrained\",\n",
    "        \"../data\",\n",
    "        \"../../pretrained_models\",\n",
    "        \"../../pretrained\",\n",
    "    ]:\n",
    "        d = (pkg_root / rel).resolve()\n",
    "        if d.exists() and d.is_dir():\n",
    "            candidates += list(d.rglob(\"*.pth\"))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda p: ((\"cnn14\" not in p.name.lower()), len(p.name)))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"No se ha encontrado ningún checkpoint .pth de PANNs CNN14.\\n\"\n",
    "    )\n",
    "\n",
    "CKPT = find_cnn14_ckpt()\n",
    "print(\"Using checkpoint:\", CKPT)\n",
    "\n",
    "panns_model = Cnn14(\n",
    "    sample_rate=TARGET_SR,\n",
    "    window_size=1024,\n",
    "    hop_size=320,\n",
    "    mel_bins=64,\n",
    "    fmin=50,\n",
    "    fmax=14000,\n",
    "    classes_num=527\n",
    ")\n",
    "\n",
    "ckpt = torch.load(CKPT, map_location=device)\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "panns_model.load_state_dict(state, strict=True)\n",
    "\n",
    "panns_model.to(device).eval()\n",
    "print(\"Loaded PANNs:\", panns_model.__class__.__name__, \"on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411656a",
   "metadata": {},
   "source": [
    "## 4. Modelo para fine-tuning\n",
    "\n",
    "Crea un wrapper que:\n",
    "- Obtiene el embedding global de PANNs (`out['embedding']`).\n",
    "- Aplica una capa `Linear` para predecir `tone_pickup`.\n",
    "\n",
    "Así podemos entrenar una cabeza ligera y, opcionalmente, descongelar una parte final del encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e9695",
   "metadata": {},
   "source": [
    "## Fine-tuning con *projection head* + adaptación parcial del backbone\n",
    "\n",
    "**Objetivo**: mejorar la calidad del espacio de embeddings para *retrieval* (Top‑1/Top‑5) manteniendo el pooling interno de PANNs, pero permitiendo una **adaptación controlada** de las **últimas capas** del encoder.\n",
    "\n",
    "**Qué cambia respecto al notebook anterior**:\n",
    "- Además de la proyección `2048 → 256`, se **descongelan** los últimos bloques convolucionales para que el modelo pueda especializarse al dominio EGFxSet (estrategia más cercana al fine‑tuning parcial aplicado en MERT).\n",
    "- El resto del protocolo (split, ventanas, evaluación por similitud coseno) se mantiene idéntico para comparar de forma directa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9069c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_dim: 2048 | proj_dim: 256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PANNsForTonePickup(nn.Module):\n",
    "    \"\"\"Wrapper de fine-tuning para PANNs.\n",
    "\n",
    "    - backbone: PANNs (CNN14) preentrenado\n",
    "    - proj: proyección entrenable para compactar el embedding (mejor retrieval)\n",
    "    - classifier: capa final de clasificación\n",
    "\n",
    "    Nota: el pooling temporal interno de PANNs NO se modifica; aquí solo se adapta el embedding final.\n",
    "    \"\"\"\n",
    "    def __init__(self, panns_backbone: nn.Module, emb_dim: int, proj_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.panns = panns_backbone\n",
    "        self.proj = nn.Linear(emb_dim, proj_dim)\n",
    "        self.classifier = nn.Linear(proj_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.panns(x, None)\n",
    "        emb_raw = out[\"embedding\"]               # (B, emb_dim)\n",
    "        emb = self.proj(emb_raw)                # (B, proj_dim)\n",
    "        emb = F.normalize(emb, p=2, dim=-1)\n",
    "        logits = self.classifier(emb)\n",
    "        return logits, emb, emb_raw\n",
    "\n",
    "panns_model.eval()\n",
    "with torch.no_grad():\n",
    "    x0, _ = next(iter(train_loader))\n",
    "    x0 = x0.to(device)\n",
    "    out0 = panns_model(x0, None)\n",
    "    emb_dim = int(out0[\"embedding\"].shape[-1])\n",
    "\n",
    "PROJ_DIM = 256\n",
    "ft_model = PANNsForTonePickup(panns_model, emb_dim=emb_dim, proj_dim=PROJ_DIM, n_classes=N_CLASSES).to(device)\n",
    "print(\"emb_dim:\", emb_dim, \"| proj_dim:\", PROJ_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e119b",
   "metadata": {},
   "source": [
    "### 5. Fine-tuning parcial de PANNs (CNN14)\n",
    "\n",
    "En este experimento se parte del modelo CNN14 preentrenado y se realiza un fine-tuning **parcial y controlado**, entrenando:\n",
    "- La **cabeza de proyección** (2048→256) y la **capa de clasificación**.\n",
    "- Un subconjunto reducido del encoder (los **últimos bloques convolucionales**), para adaptar las representaciones al dominio específico de guitarra eléctrica sin alterar por completo el conocimiento general aprendido en AudioSet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743a5f8",
   "metadata": {},
   "source": [
    "## Política de congelación\n",
    "\n",
    "Para maximizar la comparabilidad con el fine‑tuning de MERT y evitar inestabilidad:\n",
    "- **Congelamos** la mayor parte del backbone PANNs.\n",
    "- Entrenamos siempre:\n",
    "  - `proj` (proyección 2048→256)\n",
    "  - `classifier`\n",
    "- Además, **descongelamos** únicamente los **últimos bloques** del encoder (por defecto `conv_block5` y `conv_block6`), manteniendo el resto fijo.\n",
    "\n",
    "Esta estrategia busca un compromiso entre **adaptación al dominio** y **riesgo de sobreajuste**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715792e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNFREEZE_BLOCKS: ['conv_block5', 'conv_block6'] | UNFREEZE_BN_ONLY: False\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "backbone = ft_model.panns\n",
    "\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in ft_model.proj.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in ft_model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "UNFREEZE_BLOCKS = [\"conv_block5\", \"conv_block6\"]\n",
    "UNFREEZE_BN_ONLY = False\n",
    "\n",
    "def _set_trainable(module: nn.Module, trainable: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "for name in UNFREEZE_BLOCKS:\n",
    "    if not hasattr(backbone, name):\n",
    "        raise AttributeError(f\"Backbone no tiene atributo '{name}'. Revisa nombres disponibles en backbone.\")\n",
    "    block = getattr(backbone, name)\n",
    "    if UNFREEZE_BN_ONLY:\n",
    "\n",
    "        for m in block.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = True\n",
    "    else:\n",
    "        _set_trainable(block, True)\n",
    "\n",
    "print(\"UNFREEZE_BLOCKS:\", UNFREEZE_BLOCKS, \"| UNFREEZE_BN_ONLY:\", UNFREEZE_BN_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5073c80-2a77-4bab-9f4f-0172025ab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable total: 71332417\n",
      "Trainable classifier: 16705 / 16705\n",
      "Trainable conv_block6: 56631296 / 56631296\n",
      "Otros entrenables fuera de conv_block6+classifier: ['panns.conv_block5.conv1.weight', 'panns.conv_block5.conv2.weight', 'panns.conv_block5.bn1.weight', 'panns.conv_block5.bn1.bias', 'panns.conv_block5.bn2.weight', 'panns.conv_block5.bn2.bias', 'proj.weight', 'proj.bias']  (n= 8 )\n"
     ]
    }
   ],
   "source": [
    "def count_params(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def count_trainable_params(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Trainable total:\", sum(p.numel() for p in ft_model.parameters() if p.requires_grad))\n",
    "\n",
    "print(\"Trainable classifier:\", count_trainable_params(ft_model.classifier), \"/\", count_params(ft_model.classifier))\n",
    "\n",
    "print(\"Trainable conv_block6:\", count_trainable_params(ft_model.panns.conv_block6), \"/\", count_params(ft_model.panns.conv_block6))\n",
    "\n",
    "others = []\n",
    "for name, p in ft_model.named_parameters():\n",
    "    if p.requires_grad and not (name.startswith(\"panns.conv_block6.\") or name.startswith(\"classifier.\")):\n",
    "        others.append(name)\n",
    "print(\"Otros entrenables fuera de conv_block6+classifier:\", others[:20], \" (n=\", len(others), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229201d0",
   "metadata": {},
   "source": [
    "## 6. Loop de entrenamiento\n",
    "\n",
    "Entrena con `CrossEntropyLoss`, early stopping por `val_loss` y guarda el mejor estado.\n",
    "\n",
    "También mide el **tiempo total de entrenamiento** (wall-clock)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81d804",
   "metadata": {},
   "source": [
    "## Control explícito de `train()` / `eval()`\n",
    "\n",
    "Aunque el backbone esté congelado (sin gradiente), **BatchNorm/Dropout** pueden comportarse distinto según `train()`/`eval()`.\n",
    "Aquí dejamos el backbone global en `eval()` para evitar deriva de estadísticos, y si `UNFREEZE_BN_BLOCK6=True` activamos solo `conv_block6` en `train()` para permitir adaptación de BN en ese bloque.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25deb5d9",
   "metadata": {
    "tags": [
     "FIXED"
    ]
   },
   "outputs": [],
   "source": [
    "backbone = ft_model.panns\n",
    "\n",
    "backbone.eval()\n",
    "ft_model.proj.train()\n",
    "ft_model.classifier.train()\n",
    "\n",
    "if 'UNFREEZE_BN_BLOCK6' in globals() and UNFREEZE_BN_BLOCK6:\n",
    "    backbone.conv_block6.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87386a4",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Entrenamos únicamente los parámetros marcados como entrenables (proyección + classifier + BN opcional).\n",
    "En cada época fijamos el modo de trabajo:\n",
    "- backbone en `eval()`\n",
    "- `proj` y `classifier` en `train()`\n",
    "- (opcional) `conv_block6` en `train()` si queremos adaptar BN.\n",
    "\n",
    "Además, como el `forward` ahora devuelve `(logits, emb_proj, emb_raw)`, usamos `logits` para la pérdida y mantenemos `emb_proj` para extracción posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22908313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros entrenables (head): 541,249\n",
      "Parámetros entrenables (backbone): 70,791,168\n",
      "Parámetros entrenables (total): 71,332,417 / 82,378,320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:59<00:00,  7.49it/s]\n",
      "Epoch 1/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:10<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=4.0166 train_acc=0.2138 | val_loss=3.8376 val_acc=0.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:54<00:00,  8.29it/s]\n",
      "Epoch 2/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=3.7157 train_acc=0.4802 | val_loss=3.5703 val_acc=0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:53<00:00,  8.33it/s]\n",
      "Epoch 3/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=3.4653 train_acc=0.5894 | val_loss=3.3278 val_acc=0.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:53<00:00,  8.40it/s]\n",
      "Epoch 4/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss=3.2302 train_acc=0.6490 | val_loss=3.0999 val_acc=0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:54<00:00,  8.15it/s]\n",
      "Epoch 5/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss=3.0070 train_acc=0.6946 | val_loss=2.8823 val_acc=0.7140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:57<00:00,  7.75it/s]\n",
      "Epoch 6/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss=2.7946 train_acc=0.7387 | val_loss=2.6776 val_acc=0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:02<00:00,  7.12it/s]\n",
      "Epoch 7/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss=2.5911 train_acc=0.7714 | val_loss=2.4836 val_acc=0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:06<00:00,  6.71it/s]\n",
      "Epoch 8/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss=2.4006 train_acc=0.7878 | val_loss=2.2952 val_acc=0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:02<00:00,  7.22it/s]\n",
      "Epoch 9/30 [val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss=2.2152 train_acc=0.8151 | val_loss=2.1187 val_acc=0.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:01<00:00,  7.27it/s]\n",
      "Epoch 10/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_loss=2.0436 train_acc=0.8393 | val_loss=1.9526 val_acc=0.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:00<00:00,  7.39it/s]\n",
      "Epoch 11/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_loss=1.8837 train_acc=0.8561 | val_loss=1.7991 val_acc=0.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:58<00:00,  7.65it/s]\n",
      "Epoch 12/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_loss=1.7348 train_acc=0.8671 | val_loss=1.6551 val_acc=0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:57<00:00,  7.76it/s]\n",
      "Epoch 13/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_loss=1.5921 train_acc=0.8825 | val_loss=1.5192 val_acc=0.8821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [00:58<00:00,  7.63it/s]\n",
      "Epoch 14/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_loss=1.4577 train_acc=0.9015 | val_loss=1.3927 val_acc=0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:00<00:00,  7.36it/s]\n",
      "Epoch 15/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_loss=1.3329 train_acc=0.9032 | val_loss=1.2763 val_acc=0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:04<00:00,  6.89it/s]\n",
      "Epoch 16/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:10<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_loss=1.2185 train_acc=0.9201 | val_loss=1.1695 val_acc=0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:06<00:00,  6.79it/s]\n",
      "Epoch 17/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_loss=1.1090 train_acc=0.9241 | val_loss=1.0678 val_acc=0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:04<00:00,  6.90it/s]\n",
      "Epoch 18/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train_loss=1.0110 train_acc=0.9328 | val_loss=0.9766 val_acc=0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:01<00:00,  7.30it/s]\n",
      "Epoch 19/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train_loss=0.9218 train_acc=0.9381 | val_loss=0.8898 val_acc=0.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:02<00:00,  7.12it/s]\n",
      "Epoch 20/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_loss=0.8360 train_acc=0.9497 | val_loss=0.8102 val_acc=0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:00<00:00,  7.41it/s]\n",
      "Epoch 21/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:08<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train_loss=0.7572 train_acc=0.9497 | val_loss=0.7421 val_acc=0.9291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:01<00:00,  7.31it/s]\n",
      "Epoch 22/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train_loss=0.6830 train_acc=0.9553 | val_loss=0.6736 val_acc=0.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:02<00:00,  7.20it/s]\n",
      "Epoch 23/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: train_loss=0.6155 train_acc=0.9591 | val_loss=0.6100 val_acc=0.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:01<00:00,  7.24it/s]\n",
      "Epoch 24/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: train_loss=0.5539 train_acc=0.9638 | val_loss=0.5551 val_acc=0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:03<00:00,  7.07it/s]\n",
      "Epoch 25/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: train_loss=0.4969 train_acc=0.9704 | val_loss=0.5060 val_acc=0.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:03<00:00,  7.02it/s]\n",
      "Epoch 26/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: train_loss=0.4437 train_acc=0.9736 | val_loss=0.4577 val_acc=0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:04<00:00,  6.91it/s]\n",
      "Epoch 27/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: train_loss=0.3926 train_acc=0.9778 | val_loss=0.4184 val_acc=0.9413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:06<00:00,  6.75it/s]\n",
      "Epoch 28/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: train_loss=0.3496 train_acc=0.9803 | val_loss=0.3815 val_acc=0.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:05<00:00,  6.82it/s]\n",
      "Epoch 29/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: train_loss=0.3083 train_acc=0.9849 | val_loss=0.3509 val_acc=0.9464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [train]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 448/448 [01:07<00:00,  6.61it/s]\n",
      "Epoch 30/30 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:09<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: train_loss=0.2730 train_acc=0.9866 | val_loss=0.3140 val_acc=0.9531\n",
      "Training time: 35.30 min (2118.2 s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "LR_HEAD = 1e-4\n",
    "LR_BACKBONE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS_MAX = 30\n",
    "PATIENCE = 6\n",
    "GRAD_CLIP = 1.0\n",
    "MIN_DELTA = 1e-3\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = -1\n",
    "no_improve = 0\n",
    "\n",
    "head_params = [p for p in list(ft_model.proj.parameters()) + list(ft_model.classifier.parameters()) if p.requires_grad]\n",
    "backbone_params = [p for p in ft_model.panns.parameters() if p.requires_grad]\n",
    "\n",
    "print(f\"Parámetros entrenables (head): {sum(p.numel() for p in head_params):,}\")\n",
    "print(f\"Parámetros entrenables (backbone): {sum(p.numel() for p in backbone_params):,}\")\n",
    "print(f\"Parámetros entrenables (total): {sum(p.numel() for p in ft_model.parameters() if p.requires_grad):,} / {sum(p.numel() for p in ft_model.parameters()):,}\")\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": head_params, \"lr\": LR_HEAD, \"weight_decay\": WEIGHT_DECAY},\n",
    "]\n",
    "if len(backbone_params) > 0:\n",
    "    param_groups.append({\"params\": backbone_params, \"lr\": LR_BACKBONE, \"weight_decay\": WEIGHT_DECAY})\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups)\n",
    "\n",
    "history = []\n",
    "t0 = time.time()\n",
    "\n",
    "def _set_backbone_train_mode():\n",
    "\n",
    "    ft_model.panns.eval()\n",
    "    for name in UNFREEZE_BLOCKS:\n",
    "        block = getattr(ft_model.panns, name)\n",
    "        block.train()\n",
    "\n",
    "for epoch in range(1, EPOCHS_MAX + 1):\n",
    "\n",
    "    ft_model.train()\n",
    "    _set_backbone_train_mode()\n",
    "    ft_model.proj.train()\n",
    "    ft_model.classifier.train()\n",
    "\n",
    "    tr_loss = 0.0\n",
    "    tr_correct = 0\n",
    "    tr_n = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS_MAX} [train]\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits, emb, emb_raw = ft_model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        if GRAD_CLIP is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(ft_model.parameters(), GRAD_CLIP)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += float(loss.item()) * x.size(0)\n",
    "        tr_correct += int((logits.argmax(dim=1) == y).sum().item())\n",
    "        tr_n += x.size(0)\n",
    "\n",
    "    tr_loss /= max(1, tr_n)\n",
    "    tr_acc = tr_correct / max(1, tr_n)\n",
    "\n",
    "\n",
    "    ft_model.eval()\n",
    "    va_loss = 0.0\n",
    "    va_correct = 0\n",
    "    va_n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS_MAX} [val]\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, emb, emb_raw = ft_model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            va_loss += float(loss.item()) * x.size(0)\n",
    "            va_correct += int((logits.argmax(dim=1) == y).sum().item())\n",
    "            va_n += x.size(0)\n",
    "\n",
    "    va_loss /= max(1, va_n)\n",
    "    va_acc = va_correct / max(1, va_n)\n",
    "\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"train_acc\": tr_acc, \"val_loss\": va_loss, \"val_acc\": va_acc})\n",
    "    print(f\"Epoch {epoch}: train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | val_loss={va_loss:.4f} val_acc={va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val - MIN_DELTA:\n",
    "        best_val = va_loss\n",
    "        best_epoch = epoch\n",
    "        best_state = copy.deepcopy(ft_model.state_dict())\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best epoch={best_epoch}, best val_loss={best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "train_time_s = time.time() - t0\n",
    "print(f\"Training time: {train_time_s/60:.2f} min ({train_time_s:.1f} s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bbf78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/dtenreiro/Documents/TFM/panns_inference/outputs_03_panns_finetune_egfxset_unfreeze/train_history.csv\n",
      "Saved best: /Users/dtenreiro/Documents/TFM/panns_inference/outputs_03_panns_finetune_egfxset_unfreeze/best.pt\n",
      "Saved last: /Users/dtenreiro/Documents/TFM/panns_inference/outputs_03_panns_finetune_egfxset_unfreeze/last.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "HIST_CSV = OUT_DIR_03 / \"train_history.csv\"\n",
    "pd.DataFrame(history).to_csv(HIST_CSV, index=False)\n",
    "print(\"Saved:\", HIST_CSV.resolve())\n",
    "\n",
    "BEST_PT = OUT_DIR_03 / \"best.pt\"\n",
    "LAST_PT = OUT_DIR_03 / \"last.pt\"\n",
    "\n",
    "if best_state is not None:\n",
    "    ft_model.load_state_dict(best_state)\n",
    "    torch.save({\n",
    "        \"state_dict\": best_state,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_loss\": best_val,\n",
    "        \"unfreeze_blocks\": UNFREEZE_BLOCKS,\n",
    "        \"unfreeze_bn_only\": UNFREEZE_BN_ONLY,\n",
    "        \"lr_head\": LR_HEAD,\n",
    "        \"lr_backbone\": LR_BACKBONE,\n",
    "    }, BEST_PT)\n",
    "    print(\"Saved best:\", BEST_PT.resolve())\n",
    "else:\n",
    "    print(\"WARNING: best_state es None (no se guardó best.pt)\")\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": ft_model.state_dict(),\n",
    "    \"epoch_end\": history[-1][\"epoch\"] if len(history) else None,\n",
    "    \"unfreeze_blocks\": UNFREEZE_BLOCKS,\n",
    "    \"unfreeze_bn_only\": UNFREEZE_BN_ONLY,\n",
    "}, LAST_PT)\n",
    "print(\"Saved last:\", LAST_PT.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mert310)",
   "language": "python",
   "name": "mert310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
