{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5617abab-1bdd-40a1-a5e1-3fd00a2879da",
   "metadata": {},
   "source": [
    "1. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bac2ed-d4d2-49e1-96a9-6276253d3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fdef65-ce07-4e3b-b451-26708308b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590a04c3-f08e-4d97-9af2-a72f89676ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps | torch: 2.9.1 | seed: 42\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"device:\", device, \"| torch:\", torch.__version__, \"| seed:\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce4337-320a-4748-85c1-a783d9526ae1",
   "metadata": {},
   "source": [
    "2. Carga del .csv del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a058d281-f271-431b-a9a9-8832237a7db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado\n",
      "Número de muestras: 8947\n",
      "Columnas: ['path', 'tone', 'pickup', 'tone_pickup', 'string', 'fret', 'midi_pitch']\n",
      "\n",
      "                                                path         tone  pickup  \\\n",
      "0  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
      "1  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
      "2  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
      "3  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
      "4  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
      "\n",
      "           tone_pickup  string  fret  midi_pitch  \n",
      "0  BluesDriver__Bridge       1     0          64  \n",
      "1  BluesDriver__Bridge       1     1          65  \n",
      "2  BluesDriver__Bridge       1    10          74  \n",
      "3  BluesDriver__Bridge       1    11          75  \n",
      "4  BluesDriver__Bridge       1    12          76  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INDEX_CSV_PATH = Path(\"./outputs_01_mert_base_egfxset/egfxset_index.csv\")\n",
    "\n",
    "assert INDEX_CSV_PATH.exists(), f\"No existe el índice: {INDEX_CSV_PATH}\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV_PATH)\n",
    "\n",
    "print(\"Dataset cargado\")\n",
    "print(\"Número de muestras:\", len(df))\n",
    "print(\"Columnas:\", list(df.columns))\n",
    "print()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469405e-6b59-4fc7-a076-a9baf39d305f",
   "metadata": {},
   "source": [
    "3. Split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2297222-4389-4625-9673-3acb66df19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split OK\n",
      "Train: 7157 | Val: 1790\n",
      "Clases train: 65 | Clases val: 65\n",
      "\n",
      "Distribución (train):\n",
      "tone_pickup\n",
      "Chorus__Bridge               111\n",
      "TapeEcho__Neck               111\n",
      "BluesDriver__Neck            111\n",
      "Sweep Echo__Middle-Neck      111\n",
      "Clean__Middle-Neck           111\n",
      "Sweep Echo__Bridge           111\n",
      "Digital Delay__Neck          111\n",
      "TubeScreamer__Middle         111\n",
      "Sweep Echo__Bridge-Middle    111\n",
      "Plate Reverb__Bridge         111\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución (val):\n",
      "tone_pickup\n",
      "TapeEcho__Middle-Neck        28\n",
      "Spring Reverb__Neck          28\n",
      "Sweep Echo__Middle           28\n",
      "RAT__Middle-Neck             28\n",
      "TubeScreamer__Middle-Neck    28\n",
      "Flanger__Neck                28\n",
      "TapeEcho__Bridge             28\n",
      "Hall Reverb__Neck            28\n",
      "RAT__Neck                    28\n",
      "TubeScreamer__Neck           28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LABEL_COL = \"tone_pickup\"\n",
    "assert LABEL_COL in df.columns, f\"No existe la columna de etiqueta '{LABEL_COL}'. Columnas: {list(df.columns)}\"\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=df[LABEL_COL]\n",
    ")\n",
    "\n",
    "print(\"Split OK\")\n",
    "print(\"Train:\", len(train_df), \"| Val:\", len(val_df))\n",
    "print(\"Clases train:\", train_df[LABEL_COL].nunique(), \"| Clases val:\", val_df[LABEL_COL].nunique())\n",
    "print(\"\\nDistribución (train):\")\n",
    "print(train_df[LABEL_COL].value_counts().head(10))\n",
    "print(\"\\nDistribución (val):\")\n",
    "print(val_df[LABEL_COL].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccf106-426a-46a2-88c5-9af8522109ac",
   "metadata": {},
   "source": [
    "4. Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12779634-6078-4137-b7e7-b89031863eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding OK\n",
      "Num clases: 65\n",
      "Ejemplo mapping:\n",
      "0 -> BluesDriver__Bridge\n",
      "1 -> BluesDriver__Bridge-Middle\n",
      "2 -> BluesDriver__Middle\n",
      "3 -> BluesDriver__Middle-Neck\n",
      "4 -> BluesDriver__Neck\n",
      "5 -> Chorus__Bridge\n",
      "6 -> Chorus__Bridge-Middle\n",
      "7 -> Chorus__Middle\n",
      "8 -> Chorus__Middle-Neck\n",
      "9 -> Chorus__Neck\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LABEL_COL = \"tone_pickup\"\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df[LABEL_COL])\n",
    "\n",
    "train_df = train_df.copy()\n",
    "val_df   = val_df.copy()\n",
    "\n",
    "train_df[\"label_id\"] = le.transform(train_df[LABEL_COL])\n",
    "val_df[\"label_id\"]   = le.transform(val_df[LABEL_COL])\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(\"Label encoding OK\")\n",
    "print(\"Num clases:\", num_classes)\n",
    "print(\"Ejemplo mapping:\")\n",
    "for i, c in enumerate(le.classes_[:10]):\n",
    "    print(f\"{i} -> {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dd680-44da-4d59-a435-977696de5198",
   "metadata": {},
   "source": [
    "5. MERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae090079-e8ff-4eec-90f1-0a4a886899e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mert310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MERT model: m-a-p/MERT-v1-330M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-330M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-330M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERT loaded on device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "# Modelo MERT baseline\n",
    "MERT_MODEL_NAME = \"m-a-p/MERT-v1-330M\"\n",
    "print(\"Loading MERT model:\", MERT_MODEL_NAME)\n",
    "\n",
    "mert_model = AutoModel.from_pretrained(\n",
    "    MERT_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# Processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MERT_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# hidden_states\n",
    "mert_model.config.output_hidden_states = True\n",
    "\n",
    "# Config audio\n",
    "MAX_SECONDS = 5.0\n",
    "TARGET_SR = 24000\n",
    "\n",
    "def load_audio(path: str | Path):\n",
    "    \"\"\"Carga wav -> mono -> resample a 24k -> recorta a 5s.\"\"\"\n",
    "    wav, sr = sf.read(str(path))\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "    wav = torch.from_numpy(wav).float()\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "        sr = TARGET_SR\n",
    "    wav = wav[: int(sr * MAX_SECONDS)]\n",
    "    return wav, sr\n",
    "\n",
    "print(\"MERT loaded on device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d0e8c-4c79-43bb-b90a-c96720bbcc68",
   "metadata": {},
   "source": [
    "6. Data Set y Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6769ac-4c12-4b2d-a399-83a13cebe9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_SECONDS = 5.0\n",
    "TARGET_SR = 24000\n",
    "TARGET_LEN = int(TARGET_SR * MAX_SECONDS)\n",
    "\n",
    "def load_audio(path: str | Path):\n",
    "    wav, sr = sf.read(str(path))\n",
    "\n",
    "    if isinstance(wav, np.ndarray) and wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "\n",
    "    wav = torch.tensor(wav, dtype=torch.float32)\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "        sr = TARGET_SR\n",
    "\n",
    "    wav = torch.nan_to_num(wav, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if wav.numel() < TARGET_LEN:\n",
    "        pad = TARGET_LEN - wav.numel()\n",
    "        wav = torch.cat([wav, torch.zeros(pad, dtype=wav.dtype)], dim=0)\n",
    "    else:\n",
    "        wav = wav[:TARGET_LEN]\n",
    "\n",
    "    return wav, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a5f11-25c0-40b3-8f60-329e75eb9961",
   "metadata": {},
   "source": [
    "7. input_values está saliendo con shape (1, 8, 120000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debf62c7-dc2c-4d68-9361-f72284bdf57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "N_LAST_LAYERS = 4\n",
    "MAX_SECONDS = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109d10f5-073c-4196-92b0-3c154c49cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    wavs, srs, ys = zip(*batch)\n",
    "\n",
    "    wavs_np = [w.cpu().numpy().astype(np.float32) for w in wavs]\n",
    "\n",
    "    inputs = processor(\n",
    "        wavs_np,\n",
    "        sampling_rate=srs[0],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(ys, dtype=torch.long)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b9988e-b11f-4b1d-8777-8e26d17b6245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values: (8, 120000) torch.float32\n",
      "attention_mask: (8, 120000) torch.int32\n",
      "labels: (8,) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PATH_COL = \"path\"\n",
    "LABEL_ID_COL = \"label_id\"\n",
    "\n",
    "assert PATH_COL in train_df.columns, f\"Falta '{PATH_COL}' en train_df. Columnas: {list(train_df.columns)}\"\n",
    "assert LABEL_ID_COL in train_df.columns, f\"Falta '{LABEL_ID_COL}' en train_df. Columnas: {list(train_df.columns)}\"\n",
    "\n",
    "class EGFxSetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.paths = df[PATH_COL].astype(str).tolist()\n",
    "        self.labels = df[LABEL_ID_COL].astype(int).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = load_audio(self.paths[idx])\n",
    "        y = self.labels[idx]\n",
    "        return wav, sr, y\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    EGFxSetDataset(train_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    EGFxSetDataset(val_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "inputs, labels = next(iter(train_loader))\n",
    "print(\"input_values:\", tuple(inputs[\"input_values\"].shape), inputs[\"input_values\"].dtype)\n",
    "print(\"attention_mask:\", tuple(inputs[\"attention_mask\"].shape), inputs[\"attention_mask\"].dtype)\n",
    "print(\"labels:\", tuple(labels.shape), labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae9bbf3-eb69-4eb0-a31d-bc1103309cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper OK: last layer + MAX pooling\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MERTForTonePickup(nn.Module):\n",
    "    def __init__(self, mert_model, num_classes: int, proj_dim: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mert = mert_model\n",
    "\n",
    "        hidden_size = getattr(self.mert.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"No se pudo leer hidden_size de mert_model.config.hidden_size\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(hidden_size, proj_dim)\n",
    "        self.classifier = nn.Linear(proj_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        out = self.mert(**inputs)\n",
    "        h = out.last_hidden_state  # (B, T, H)\n",
    "\n",
    "        # MAX pooling temporal\n",
    "        emb = h.max(dim=1).values  # (B, H)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        emb = F.relu(self.proj(emb))  # (B, proj_dim)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        logits = self.classifier(emb)\n",
    "        loss = F.cross_entropy(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits, \"emb\": emb}\n",
    "\n",
    "ft_model = MERTForTonePickup(mert_model, num_classes=num_classes, proj_dim=256, dropout=0.1).to(device)\n",
    "\n",
    "print(\"Wrapper OK: last layer + MAX pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0768fa-2fe6-4c43-b5db-b8e0d747d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze last 4/24 layers via 'encoder.layers' + head/proj\n",
      "Trainable params: 50,664,001 / 315,708,097 (16.05%)\n"
     ]
    }
   ],
   "source": [
    "N_LAST_LAYERS = 4\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "for p in ft_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in ft_model.proj.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in ft_model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "mert_core = ft_model.mert\n",
    "\n",
    "layer_stack = None\n",
    "candidates = [\n",
    "    (\"encoder.layers\", lambda m: m.encoder.layers),\n",
    "    (\"model.encoder.layers\", lambda m: m.model.encoder.layers),\n",
    "    (\"transformer.layers\", lambda m: m.transformer.layers),\n",
    "    (\"model.layers\", lambda m: m.model.layers),\n",
    "    (\"layers\", lambda m: m.layers),\n",
    "]\n",
    "\n",
    "for name, getter in candidates:\n",
    "    try:\n",
    "        ls = getter(mert_core)\n",
    "        if hasattr(ls, \"__len__\"):\n",
    "            layer_stack = ls\n",
    "            stack_name = name\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "assert layer_stack is not None, (\n",
    "    \"No pude encontrar el stack de capas del encoder en este MERT\"\n",
    ")\n",
    "\n",
    "num_layers = len(layer_stack)\n",
    "n = min(N_LAST_LAYERS, num_layers)\n",
    "\n",
    "for layer in layer_stack[num_layers - n:]:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in ft_model.parameters() if p.requires_grad],\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in ft_model.parameters())\n",
    "print(f\"Unfroze last {n}/{num_layers} layers via '{stack_name}' + head/proj\")\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({trainable/total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c48c6e-8160-484c-8c22-ff185792adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:29<00:00,  1.04s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:48<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train loss 4.0357 acc 0.038 | val loss 3.7734 acc 0.081\n",
      "new best val_loss=3.7734 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:50<00:00,  1.06s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:51<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 | train loss 3.3683 acc 0.137 | val loss 2.9204 acc 0.265\n",
      "new best val_loss=2.9204 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:35<00:00,  1.05s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:46<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03 | train loss 2.5124 acc 0.316 | val loss 2.2208 acc 0.439\n",
      "new best val_loss=2.2208 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:25<00:00,  1.03s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:40<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04 | train loss 1.8480 acc 0.487 | val loss 1.6856 acc 0.567\n",
      "new best val_loss=1.6856 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [16:12<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:48<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05 | train loss 1.3231 acc 0.632 | val loss 1.2643 acc 0.674\n",
      "new best val_loss=1.2643 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [16:02<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:41<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06 | train loss 0.9339 acc 0.752 | val loss 0.9227 acc 0.766\n",
      "new best val_loss=0.9227 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:43<00:00,  1.05s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:41<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07 | train loss 0.6425 acc 0.831 | val loss 0.7410 acc 0.794\n",
      "new best val_loss=0.7410 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:03<00:00,  1.01s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:36<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08 | train loss 0.4442 acc 0.877 | val loss 0.5804 acc 0.826\n",
      "new best val_loss=0.5804 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:49<00:00,  1.01it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09 | train loss 0.3138 acc 0.912 | val loss 0.5072 acc 0.830\n",
      "new best val_loss=0.5072 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:32<00:00,  1.04s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:36<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | train loss 0.2274 acc 0.934 | val loss 0.4623 acc 0.846\n",
      "new best val_loss=0.4623 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:28<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:35<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | train loss 0.1663 acc 0.950 | val loss 0.4301 acc 0.855\n",
      "new best val_loss=0.4301 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:28<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:35<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | train loss 0.1182 acc 0.965 | val loss 0.3870 acc 0.882\n",
      "new best val_loss=0.3870 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:38<00:00,  1.02it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:41<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | train loss 0.0770 acc 0.979 | val loss 0.4095 acc 0.874\n",
      "no improvement | patience left 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:46<00:00,  1.01it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:32<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | train loss 0.0482 acc 0.986 | val loss 0.3417 acc 0.901\n",
      "new best val_loss=0.3417 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:45<00:00,  1.01it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:39<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | train loss 0.0405 acc 0.988 | val loss 0.3648 acc 0.895\n",
      "no improvement | patience left 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:56<00:00,  1.00s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:40<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | train loss 0.0290 acc 0.992 | val loss 0.2920 acc 0.916\n",
      "new best val_loss=0.2920 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [15:06<00:00,  1.01s/it]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:37<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | train loss 0.0272 acc 0.991 | val loss 0.2776 acc 0.923\n",
      "new best val_loss=0.2776 -> saved outputs_02_mert_finetune_egfxset/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:37<00:00,  1.02it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:37<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | train loss 0.0271 acc 0.992 | val loss 0.3822 acc 0.903\n",
      "no improvement | patience left 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:48<00:00,  1.01it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:35<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | train loss 0.0208 acc 0.994 | val loss 0.5762 acc 0.864\n",
      "no improvement | patience left 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [14:27<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████| 224/224 [01:35<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | train loss 0.0198 acc 0.995 | val loss 0.3704 acc 0.908\n",
      "no improvement | patience left 0/3\n",
      "Early stopping\n",
      "Training done.\n",
      "best_val_loss: 0.27758973892830185\n",
      "Best ckpt: outputs_02_mert_finetune_egfxset/best.pt\n",
      "Last ckpt: outputs_02_mert_finetune_egfxset/last.pt\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "EPOCHS_MAX = 30\n",
    "PATIENCE = 3\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")\n",
    "OUT_DIR_02.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_CKPT = OUT_DIR_02 / \"best.pt\"\n",
    "LAST_CKPT = OUT_DIR_02 / \"last.pt\"\n",
    "\n",
    "def run_epoch(loader, train: bool):\n",
    "    ft_model.train(train)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for inputs, labels in tqdm(loader, disable=False):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = ft_model(inputs, labels=labels)\n",
    "        loss = out[\"loss\"]\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ft_model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * labels.size(0)\n",
    "        preds = out[\"logits\"].argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "history_ft = globals().get(\"history_ft\", [])\n",
    "\n",
    "best_val_loss = math.inf\n",
    "pat_left = PATIENCE\n",
    "start_epoch = 1\n",
    "\n",
    "if LAST_CKPT.exists():\n",
    "    ckpt_last = torch.load(LAST_CKPT, map_location=\"cpu\")\n",
    "    if \"ft_model_state_dict\" in ckpt_last:\n",
    "        ft_model.load_state_dict(ckpt_last[\"ft_model_state_dict\"], strict=True)\n",
    "    if \"optimizer_state_dict\" in ckpt_last:\n",
    "        try:\n",
    "            optimizer.load_state_dict(ckpt_last[\"optimizer_state_dict\"])\n",
    "            print(f\"Resumed optimizer state from {LAST_CKPT}\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: could not load optimizer state: {e}\")\n",
    "    if \"history_ft\" in ckpt_last:\n",
    "        history_ft = ckpt_last[\"history_ft\"]\n",
    "    if \"epoch\" in ckpt_last:\n",
    "        start_epoch = int(ckpt_last[\"epoch\"]) + 1\n",
    "    if \"best_val_loss\" in ckpt_last:\n",
    "        best_val_loss = float(ckpt_last[\"best_val_loss\"])\n",
    "    if \"pat_left\" in ckpt_last:\n",
    "        pat_left = int(ckpt_last[\"pat_left\"])\n",
    "\n",
    "    print(f\"Resumed model from {LAST_CKPT} | next epoch: {start_epoch} | best_val_loss: {best_val_loss:.4f} | pat_left: {pat_left}\")\n",
    "else:\n",
    "\n",
    "    if BEST_CKPT.exists():\n",
    "        ckpt_best = torch.load(BEST_CKPT, map_location=\"cpu\")\n",
    "        if \"best_val_loss\" in ckpt_best:\n",
    "            best_val_loss = float(ckpt_best[\"best_val_loss\"])\n",
    "        print(f\"Loaded best_val_loss={best_val_loss:.4f} from {BEST_CKPT}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS_MAX + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
    "    history_ft.append((epoch, tr_loss, tr_acc, va_loss, va_acc))\n",
    "\n",
    "    print(f\"epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"ft_model_state_dict\": ft_model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"history_ft\": history_ft,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"pat_left\": pat_left,\n",
    "        },\n",
    "        LAST_CKPT\n",
    "    )\n",
    "\n",
    "    if va_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = va_loss\n",
    "        pat_left = PATIENCE\n",
    "        torch.save(\n",
    "            {\n",
    "                \"ft_model_state_dict\": ft_model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"history_ft\": history_ft,\n",
    "                \"pat_left\": pat_left,\n",
    "                \"label_classes\": le.classes_.tolist(),\n",
    "                \"n_last_layers\": int(N_LAST_LAYERS),\n",
    "                \"mert_model_name\": MERT_MODEL_NAME,\n",
    "                \"proj_dim\": 256,\n",
    "                \"pooling\": \"max_last_hidden_state\",\n",
    "            },\n",
    "            BEST_CKPT\n",
    "        )\n",
    "        print(f\"new best val_loss={best_val_loss:.4f} -> saved {BEST_CKPT}\")\n",
    "    else:\n",
    "        pat_left -= 1\n",
    "        print(f\"no improvement | patience left {pat_left}/{PATIENCE}\")\n",
    "        if pat_left == 0:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "print(\"Training done.\")\n",
    "print(\"best_val_loss:\", best_val_loss)\n",
    "print(\"Best ckpt:\", BEST_CKPT)\n",
    "print(\"Last ckpt:\", LAST_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fb11b-739c-43c4-9215-b03e372c406e",
   "metadata": {},
   "source": [
    "10. Se guardan los pesos y metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61df64bd-e80a-4815-8d88-5d10cd2cd8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - outputs_02_mert_finetune_egfxset/mert_ft_last4layers_headproj.pt\n",
      " - outputs_02_mert_finetune_egfxset/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")\n",
    "OUT_DIR_02.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_path = OUT_DIR_02 / \"mert_ft_last4layers_headproj.pt\"\n",
    "meta_path = OUT_DIR_02 / \"metadata.json\"\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"ft_model_state_dict\": ft_model.state_dict(),\n",
    "        \"label_classes\": le.classes_.tolist(),\n",
    "        \"label_col\": LABEL_COL,\n",
    "        \"path_col\": PATH_COL,\n",
    "        \"num_classes\": int(num_classes),\n",
    "        \"seed\": int(SEED),\n",
    "        \"n_last_layers\": int(N_LAST_LAYERS),\n",
    "    },\n",
    "    ckpt_path\n",
    ")\n",
    "\n",
    "meta = {\n",
    "    \"mert_model_name\": MERT_MODEL_NAME,\n",
    "    \"target_sr\": TARGET_SR,\n",
    "    \"max_seconds\": MAX_SECONDS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"n_last_layers\": N_LAST_LAYERS,\n",
    "}\n",
    "if \"history_ft\" in globals():\n",
    "    meta[\"history_ft\"] = [\n",
    "        {\"epoch\": int(e), \"train_loss\": float(tl), \"train_acc\": float(ta), \"val_loss\": float(vl), \"val_acc\": float(va)}\n",
    "        for (e, tl, ta, vl, va) in history_ft\n",
    "    ]\n",
    "\n",
    "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ckpt_path)\n",
    "print(\" -\", meta_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mert310)",
   "language": "python",
   "name": "mert310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
