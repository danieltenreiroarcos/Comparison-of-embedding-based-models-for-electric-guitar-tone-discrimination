{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddeee9cb",
   "metadata": {},
   "source": [
    "# Fine-tuned Checkpoint + Embeddings\n",
    "\n",
    "Notebook de preparación de artefactos. Carga el checkpoint fine-tuned y genera embeddings de `train` y `val`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5030b-f7f9-4e57-bddd-f93ecb1611cd",
   "metadata": {},
   "source": [
    "1. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27228c4-9363-44a6-9d91-2941e102ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps | torch: 2.9.1 | seed: 42\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"device:\", device, \"| torch:\", torch.__version__, \"| seed:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3d614-3bdc-4ed2-bc47-eab91ac4158f",
   "metadata": {},
   "source": [
    "2. Índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec10a12-271a-46ac-96ab-bcb21651a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado\n",
      "Número de muestras: 8947\n",
      "Columnas: ['path', 'tone', 'pickup', 'tone_pickup', 'string', 'fret', 'midi_pitch']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>tone</th>\n",
       "      <th>pickup</th>\n",
       "      <th>tone_pickup</th>\n",
       "      <th>string</th>\n",
       "      <th>fret</th>\n",
       "      <th>midi_pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...</td>\n",
       "      <td>BluesDriver</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>BluesDriver__Bridge</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path         tone  pickup  \\\n",
       "0  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "1  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "2  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "3  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "4  /Users/dtenreiro/Documents/TFM/EGFxSet/BluesDr...  BluesDriver  Bridge   \n",
       "\n",
       "           tone_pickup  string  fret  midi_pitch  \n",
       "0  BluesDriver__Bridge       1     0          64  \n",
       "1  BluesDriver__Bridge       1     1          65  \n",
       "2  BluesDriver__Bridge       1    10          74  \n",
       "3  BluesDriver__Bridge       1    11          75  \n",
       "4  BluesDriver__Bridge       1    12          76  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INDEX_CSV_PATH = Path(\"../egfxset_index.csv\")\n",
    "assert INDEX_CSV_PATH.exists(), f\"No existe el índice: {INDEX_CSV_PATH}\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV_PATH)\n",
    "\n",
    "print(\"Dataset cargado\")\n",
    "print(\"Número de muestras:\", len(df))\n",
    "print(\"Columnas:\", list(df.columns))\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d737e-0ba3-4c38-a013-c3854bfa53bf",
   "metadata": {},
   "source": [
    "3. Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ccd1ee-8569-4a75-a133-ac140b4be36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split OK\n",
      "Train: 7157 | Val: 1790\n",
      "Clases train: 65 | Clases val: 65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LABEL_COL = \"tone_pickup\"\n",
    "PATH_COL = \"path\"\n",
    "\n",
    "assert LABEL_COL in df.columns, f\"No existe '{LABEL_COL}'. Columnas: {list(df.columns)}\"\n",
    "assert PATH_COL in df.columns, f\"No existe '{PATH_COL}'. Columnas: {list(df.columns)}\"\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=df[LABEL_COL]\n",
    ")\n",
    "\n",
    "print(\"Split OK\")\n",
    "print(\"Train:\", len(train_df), \"| Val:\", len(val_df))\n",
    "print(\"Clases train:\", train_df[LABEL_COL].nunique(), \"| Clases val:\", val_df[LABEL_COL].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce03dcb-640f-42a1-a655-4b75bffff30b",
   "metadata": {},
   "source": [
    "4. Audio loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e90b36a-48b7-460c-be5c-1f60ce7e861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_SECONDS = 5.0\n",
    "TARGET_SR = 24000\n",
    "TARGET_LEN = int(TARGET_SR * MAX_SECONDS)\n",
    "\n",
    "def load_audio(path: str | Path):\n",
    "    wav, sr = sf.read(str(path))\n",
    "\n",
    "    # mono\n",
    "    if isinstance(wav, np.ndarray) and wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "\n",
    "    # to torch float32\n",
    "    wav = torch.tensor(wav, dtype=torch.float32)\n",
    "\n",
    "    # resample\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "        sr = TARGET_SR\n",
    "\n",
    "    # sanitize\n",
    "    wav = torch.nan_to_num(wav, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # pad/trim to fixed length\n",
    "    if wav.numel() < TARGET_LEN:\n",
    "        pad = TARGET_LEN - wav.numel()\n",
    "        wav = torch.cat([wav, torch.zeros(pad, dtype=wav.dtype)], dim=0)\n",
    "    else:\n",
    "        wav = wav[:TARGET_LEN]\n",
    "\n",
    "    return wav, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f42f8-9984-4a38-880d-41a31d0467f0",
   "metadata": {},
   "source": [
    "5. MERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774e2825-beaf-4760-8d78-5def4ae77384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mert310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MERT model: m-a-p/MERT-v1-330M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-330M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-330M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERT loaded on device: mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "MERT_MODEL_NAME = \"m-a-p/MERT-v1-330M\"\n",
    "print(\"Loading MERT model:\", MERT_MODEL_NAME)\n",
    "\n",
    "mert_model = AutoModel.from_pretrained(\n",
    "    MERT_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MERT_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "mert_model.config.output_hidden_states = True\n",
    "\n",
    "print(\"MERT loaded on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0123d3-76ce-4731-8562-139f06d75bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MERTForTonePickup(nn.Module):\n",
    "    def __init__(self, mert_model, num_classes: int, proj_dim: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mert = mert_model\n",
    "\n",
    "        hidden_size = getattr(self.mert.config, \"hidden_size\", None)\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"No se pudo leer hidden_size de mert_model.config.hidden_size\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(hidden_size, proj_dim)\n",
    "        self.classifier = nn.Linear(proj_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        out = self.mert(**inputs)\n",
    "        h = out.last_hidden_state  # (B, T, H)\n",
    "\n",
    "        # MAX pooling temporal\n",
    "        emb = h.max(dim=1).values  # (B, H)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        emb = F.relu(self.proj(emb))  # (B, proj_dim)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        logits = self.classifier(emb)\n",
    "        loss = F.cross_entropy(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits, \"emb\": emb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb19300-7038-4e60-a21f-83ccc4d60cc2",
   "metadata": {},
   "source": [
    "6. Modelo fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ee7d4",
   "metadata": {},
   "source": [
    "6.1. Carga de checkpoint y mapeo de clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768954e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint parcheado guardado en: outputs_02_mert_finetune_egfxset/mert_ft_last4layers_headproj_patched_with_class_to_idx.pt\n",
      "Usando checkpoint: outputs_02_mert_finetune_egfxset/mert_ft_last4layers_headproj_patched_with_class_to_idx.pt\n",
      "Num clases: 65\n",
      "Ejemplo: [('BluesDriver__Bridge', 0), ('BluesDriver__Bridge-Middle', 1), ('BluesDriver__Middle', 2), ('BluesDriver__Middle-Neck', 3), ('BluesDriver__Neck', 4)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_PATH = OUT_DIR_02 / \"mert_ft_last4layers_headproj.pt\"\n",
    "assert CKPT_PATH.exists(), f\"No existe el checkpoint: {CKPT_PATH}\"\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "if \"class_to_idx\" in ckpt:\n",
    "    class_to_idx = ckpt[\"class_to_idx\"]\n",
    "elif \"label2id\" in ckpt:\n",
    "    class_to_idx = ckpt[\"label2id\"]\n",
    "else:\n",
    "    LABEL_COL = \"tone_pickup\"\n",
    "    classes = sorted(train_df[LABEL_COL].astype(str).unique().tolist())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    ckpt[\"class_to_idx\"] = class_to_idx\n",
    "    patched_path = CKPT_PATH.with_name(CKPT_PATH.stem + \"_patched_with_class_to_idx.pt\")\n",
    "    torch.save(ckpt, patched_path)\n",
    "    print(\"Checkpoint parcheado guardado en:\", patched_path)\n",
    "\n",
    "    CKPT_PATH = patched_path\n",
    "\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(\"Usando checkpoint:\", CKPT_PATH)\n",
    "print(\"Num clases:\", len(class_to_idx))\n",
    "print(\"Ejemplo:\", list(class_to_idx.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a23d28-4f17-43e4-8af6-0955f734259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned checkpoint: outputs_02_mert_finetune_egfxset/mert_ft_last4layers_headproj_patched_with_class_to_idx.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")\n",
    "\n",
    "assert CKPT_PATH.exists(), f\"No existe el checkpoint: {CKPT_PATH}\"\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "\n",
    "num_classes = int(ckpt[\"num_classes\"])\n",
    "\n",
    "ft_model = MERTForTonePickup(\n",
    "    mert_model,\n",
    "    num_classes=num_classes,\n",
    "    proj_dim=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "ft_model.load_state_dict(ckpt[\"ft_model_state_dict\"], strict=True)\n",
    "ft_model.eval()\n",
    "\n",
    "print(\"Loaded fine-tuned checkpoint:\", CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9890b342-1863-46b4-9a3c-c624a22c8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing (train): 0\n",
      "Missing (val): 0\n"
     ]
    }
   ],
   "source": [
    "LABEL_COL = \"tone_pickup\"\n",
    "train_classes = set(train_df[LABEL_COL].astype(str).unique())\n",
    "val_classes   = set(val_df[LABEL_COL].astype(str).unique())\n",
    "\n",
    "missing_train = train_classes - set(class_to_idx.keys())\n",
    "missing_val   = val_classes   - set(class_to_idx.keys())\n",
    "\n",
    "print(\"Missing (train):\", len(missing_train))\n",
    "print(\"Missing (val):\", len(missing_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c0725-dbad-42c8-b246-9e46950d476b",
   "metadata": {},
   "source": [
    "7. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f50cb3a-1449-4553-97cf-349b92c63b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FullDFDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.paths = df[PATH_COL].astype(str).tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = load_audio(self.paths[idx])\n",
    "        return wav, sr\n",
    "\n",
    "def collate_audio_only(batch):\n",
    "    wavs, srs = zip(*batch)\n",
    "\n",
    "    wavs_np = [w.cpu().numpy().astype(np.float32) for w in wavs]\n",
    "\n",
    "    inputs = processor(\n",
    "        wavs_np,\n",
    "        sampling_rate=srs[0],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ec3ca-b747-4c50-a0bc-4358a7baf1eb",
   "metadata": {},
   "source": [
    "8. Embeddings de Train y Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef52f7ad-3df5-48fb-9f75-8a48497ff008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 895/895 [06:34<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_02_mert_finetune_egfxset/mert_ft_embs_train.npy | shape: (7157, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 224/224 [01:54<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_02_mert_finetune_egfxset/mert_ft_embs_val.npy | shape: (1790, 256)\n",
      "Index snapshots saved:\n",
      " - outputs_02_mert_finetune_egfxset/egfxset_index_used_train.csv\n",
      " - outputs_02_mert_finetune_egfxset/egfxset_index_used_val.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "FT_TRAIN_EMBS_PATH  = OUT_DIR_02 / \"mert_ft_embs_train.npy\"\n",
    "FT_VAL_EMBS_PATH    = OUT_DIR_02 / \"mert_ft_embs_val.npy\"\n",
    "FT_TRAIN_INDEX_PATH = OUT_DIR_02 / \"egfxset_index_used_train.csv\"\n",
    "FT_VAL_INDEX_PATH   = OUT_DIR_02 / \"egfxset_index_used_val.csv\"\n",
    "\n",
    "train_df.to_csv(FT_TRAIN_INDEX_PATH, index=False)\n",
    "val_df.to_csv(FT_VAL_INDEX_PATH, index=False)\n",
    "\n",
    "def extract_embeddings_df(df_subset, out_embs_path, batch_size=8):\n",
    "    loader = DataLoader(\n",
    "        FullDFDataset(df_subset),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_audio_only\n",
    "    )\n",
    "\n",
    "    all_embs = []\n",
    "    ft_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            out = ft_model(inputs, labels=None)\n",
    "            emb = out[\"emb\"].detach().cpu().numpy()  # (B, 256)\n",
    "            all_embs.append(emb)\n",
    "\n",
    "    all_embs = np.vstack(all_embs)\n",
    "    np.save(out_embs_path, all_embs)\n",
    "    print(\"Saved:\", out_embs_path, \"| shape:\", all_embs.shape)\n",
    "    return all_embs\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "embs_train = extract_embeddings_df(train_df, FT_TRAIN_EMBS_PATH, batch_size=BATCH_SIZE)\n",
    "embs_val   = extract_embeddings_df(val_df,   FT_VAL_EMBS_PATH,   batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Index snapshots saved:\")\n",
    "print(\" -\", FT_TRAIN_INDEX_PATH)\n",
    "print(\" -\", FT_VAL_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55238d1a-6f00-4fef-b392-512a15d28247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK paths\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR_02 = Path(\"./outputs_02_mert_finetune_egfxset\")\n",
    "\n",
    "FT_TRAIN_EMBS_PATH  = OUT_DIR_02 / \"mert_ft_embs_train.npy\"\n",
    "FT_VAL_EMBS_PATH    = OUT_DIR_02 / \"mert_ft_embs_val.npy\"\n",
    "FT_TRAIN_INDEX_PATH = OUT_DIR_02 / \"egfxset_index_used_train.csv\"\n",
    "FT_VAL_INDEX_PATH   = OUT_DIR_02 / \"egfxset_index_used_val.csv\"\n",
    "\n",
    "assert FT_TRAIN_EMBS_PATH.exists()\n",
    "assert FT_VAL_EMBS_PATH.exists()\n",
    "assert FT_TRAIN_INDEX_PATH.exists()\n",
    "assert FT_VAL_INDEX_PATH.exists()\n",
    "\n",
    "print(\"OK paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7ed485-7070-40d1-a515-bf54f4b324cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7157, 256) | X_val: (1790, 256)\n",
      "num_classes: 65 | chance: 0.015384615384615385\n"
     ]
    }
   ],
   "source": [
    "LABEL_COL = \"tone_pickup\" \n",
    "\n",
    "train_df = pd.read_csv(FT_TRAIN_INDEX_PATH)\n",
    "val_df   = pd.read_csv(FT_VAL_INDEX_PATH)\n",
    "\n",
    "X_train = np.load(FT_TRAIN_EMBS_PATH)\n",
    "X_val   = np.load(FT_VAL_EMBS_PATH)\n",
    "\n",
    "assert len(train_df) == X_train.shape[0], (len(train_df), X_train.shape)\n",
    "assert len(val_df)   == X_val.shape[0],   (len(val_df), X_val.shape)\n",
    "\n",
    "# Mapeo de clases consistente entre train y val\n",
    "classes = sorted(pd.concat([train_df[LABEL_COL], val_df[LABEL_COL]]).unique().tolist())\n",
    "class2id = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "y_train = train_df[LABEL_COL].map(class2id).to_numpy()\n",
    "y_val   = val_df[LABEL_COL].map(class2id).to_numpy()\n",
    "\n",
    "num_classes = len(classes)\n",
    "chance = 1.0 / num_classes\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"| X_val:\", X_val.shape)\n",
    "print(\"num_classes:\", num_classes, \"| chance:\", chance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mert310)",
   "language": "python",
   "name": "mert310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
